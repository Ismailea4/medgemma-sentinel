[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "medgemma-sentinel"
version = "0.1.0"
description = "MedGemma Sentinel: Autonomous Edge-AI Guardian for offline rural clinics"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "MedGemma Group", email = "contact@medgemma.dev"}
]
keywords = ["healthcare", "ai", "edge-computing", "llm", "ragraph"]

dependencies = [
    # Core LangGraph & LangChain
    "langgraph>=1.0.0",
    "langchain>=1.2.0",
    "langchain-community>=0.4.0",
    
    # LlamaIndex for GraphRAG
    "llama-index-core>=0.14.0",
    "llama-index-graph-stores-neo4j>=0.3.0",
    "llama-index-llms-huggingface>=0.3.0",
    "llama-index-embeddings-huggingface>=0.3.0",
    
    # PDF Generation
    "reportlab>=4.4.0",
    "weasyprint>=62.0",
    
    # Data Processing
    "pandas>=2.2.0",
    "numpy>=2.0.0",
    "pydantic>=2.8.0",
    
    # Visualization (clinical plots: vitals trends, events timeline, severity)
    "matplotlib>=3.9.0",
    
    # Graph (embedded for offline use)
    "networkx>=3.3",
    
    # Utilities
    "python-dateutil>=2.9.0",
    "jinja2>=3.1.0",
    "typing-extensions>=4.12.0",
    
    # Local LLM Inference (offline MedGemma via GGUF)
    "llama-cpp-python>=0.3.0",
    
    # HuggingFace model loading
    "transformers>=4.44.0",
    "torch>=2.4.0",
    
    # UI
    "streamlit>=1.28.0",
    "plotly>=5.17.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.1.0",
    "black>=23.0.0",
]

[project.scripts]
medgemma-init = "scripts.init:main"
medgemma-ui = "streamlit run ui/app.py"

[tool.uv]
dev-dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=4.1.0",
]

[tool.hatch.build.targets.wheel]
packages = ["src"]
