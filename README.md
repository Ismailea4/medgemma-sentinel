# MedGemma Quantized - Local Medical AI Engine

A production-ready deployment of Google's MedGemma-1.5-4B model, quantized with medical-specific calibration data for efficient local inference on consumer hardware.

## üéØ Project Overview

This project enables running a medical AI model locally with:
- **Sub-2s response times** on 8GB RAM hardware
- **2.5GB model size** (quantized from 8.5GB using I-Matrix)
- **OpenAI-compatible API** for easy integration
- **JSON mode** for agentic workflows (LangGraph, LangChain)
- **Medical accuracy retention** via custom calibration dataset

---

## üìã Prerequisites

- **Hardware**: 8GB+ RAM (16GB recommended), Apple Silicon or NVIDIA GPU preferred
- **OS**: macOS, Linux, or Windows
- **Python**: 3.8+
- **llama.cpp**: Pre-built binary (see setup below)

---

## üöÄ Quick Start

### 1. Install Dependencies

```bash
# Activate virtual environment
source .venv/bin/activate  # or: .venv\Scripts\activate on Windows

# Install Python packages
pip install psutil requests
```

### 2. Verify Files

Ensure these files exist:
```bash
ls -lh outputs/medgemma-1.5-medical-Q4_K_M.gguf
ls -lh llama.cpp/build/bin/llama-server
```

### 3. Start the Server

```bash
python scripts/launch_server.py
```

Expected output:
```
Detected: Darwin (arm64) | RAM: 8.0 GB
Apple Silicon detected (Metal build expected).
Launching: ./llama.cpp/build/bin/llama-server -m outputs/medgemma-1.5-medical-Q4_K_M.gguf ...
main: server is listening on http://0.0.0.0:8080
```

Server is now running at: **http://localhost:8080**

---

## üíª Using the Client Library

### Import and Initialize

```python
from medgemma_client import MedGemmaAgent

agent = MedGemmaAgent()  # Defaults to http://localhost:8080
```

### Clinical Text Generation

```python
# Triage scenario
response = agent.generate_clinical_text(
    "Patient has crushing chest pain radiating to left arm.",
    system_prompt="You are an ER triage nurse. Identify primary concern.",
    max_tokens=200,
    temperature=0.2
)
print(response)
# Output: "The patient's primary concern is a potential acute myocardial 
# infarction (heart attack)..."
```

### Structured JSON Extraction (For Agents)

```python
# Extract symptoms for agent workflows
data = agent.generate_strict_json(
    "Extract symptoms: Patient has fever (39C), dry cough, and fatigue."
)
print(data)
# Output: {"symptoms": ["fever", "dry cough", "fatigue"]}
```

### Advanced Parameters

```python
response = agent.generate_clinical_text(
    prompt="What are contraindications for aspirin?",
    system_prompt="You are a clinical pharmacist.",
    max_tokens=300,
    temperature=0.1,  # Lower = more deterministic
    timeout=60        # Increase for complex queries
)
```

---

## üîå REST API Reference

The server exposes OpenAI-compatible endpoints.

### Chat Completions

**Endpoint:** `POST /v1/chat/completions`

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "medgemma",
    "messages": [
      {"role": "system", "content": "You are a medical assistant."},
      {"role": "user", "content": "What is hypertension?"}
    ],
    "temperature": 0.2,
    "max_tokens": 200
  }'
```

### Standard Completions (For JSON Mode)

**Endpoint:** `POST /completion`

```bash
curl http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Extract symptoms: Patient has headache and nausea.",
    "temperature": 0.1,
    "n_predict": 200,
    "grammar": "<json_grammar_here>"
  }'
```

---

## üß™ Testing & Verification

### Run Comprehensive Test Suite

```bash
python tests/verification_suite.py
```

This runs 10 test scenarios covering:
- Emergency triage
- Symptom extraction (JSON)
- Differential diagnosis
- Medication queries
- Pediatric cases
- Edge cases

Results are saved to `test_results/verification_results_[timestamp].txt`

### Quick Manual Test

```bash
python -c "from medgemma_client import MedGemmaAgent; a=MedGemmaAgent(); print(a.generate_clinical_text('Patient has stiff neck and fever. What is the triage concern?'))"
```

Expected: Medical response identifying meningitis as primary concern.

---

## üìÅ Project Structure

```
GemmaQuantiazed/
‚îú‚îÄ‚îÄ calibration_data/
‚îÇ   ‚îî‚îÄ‚îÄ medical_calibration_sota_v3.txt    # Custom calibration dataset
‚îú‚îÄ‚îÄ llama.cpp/                             # Inference engine (submodule)
‚îÇ   ‚îî‚îÄ‚îÄ build/bin/llama-server
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ medgemma-1.5-4b-it/               # Original HF model
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ medgemma-1.5-medical-Q4_K_M.gguf  # Quantized model (2.5GB)
‚îÇ   ‚îî‚îÄ‚îÄ medgemma-1.5-4b-it-F16.gguf       # F16 version
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ launch_server.py                   # Smart server launcher
‚îÇ   ‚îî‚îÄ‚îÄ generate_medical_calibration_v2.py # Dataset generator
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ verification_suite.py              # Comprehensive tests
‚îú‚îÄ‚îÄ medgemma_client.py                     # Python client library
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Configuration

### Server Settings (launch_server.py)

The launcher auto-detects hardware and adjusts:

| Hardware | Context Window | GPU Offload | Best For |
|----------|---------------|-------------|----------|
| Apple Silicon 8GB | 4096 | 99 layers | Standard medical queries |
| Apple Silicon 16GB+ | 8192 | 99 layers | Long reports, conversations |
| NVIDIA GPU 12GB+ | 8192 | 99 layers | High throughput |
| CPU-only 8GB | 2048 | 0 layers | Fallback mode |

### Manual Override

```bash
export LLAMA_SERVER_PATH=/custom/path/to/llama-server
python scripts/launch_server.py
```

### Client Timeout Adjustments

For complex queries (differential diagnosis, long explanations):

```python
agent.generate_clinical_text(
    prompt="...",
    timeout=90  # Increase from default 30s
)
```

---

## üîí Security Considerations

### Current Setup (Development/Demo)
- Server binds to `0.0.0.0:8080` (LAN accessible)
- No authentication required
- Suitable for: hackathons, local development, trusted networks

### Production Recommendations

1. **Bind to localhost only:**
   Edit `launch_server.py`, change `--host 0.0.0.0` to `--host 127.0.0.1`

2. **Add reverse proxy with auth:**
   ```bash
   # Example: nginx with basic auth
   nginx -> localhost:8080
   ```

3. **Use HTTPS:**
   Add `--ssl-cert-file` and `--ssl-key-file` to llama-server launch

---

## üêõ Troubleshooting

### Issue: "Could not find llama-server binary"
**Solution:** Build llama.cpp first:
```bash
cd llama.cpp
cmake -B build -DGGML_METAL=ON  # or -DGGML_CUDA=ON for NVIDIA
cmake --build build --config Release
```

### Issue: Timeouts on complex queries
**Solution:** Increase timeout in client calls:
```python
response = agent.generate_clinical_text(prompt, timeout=90)
```

### Issue: Out of memory errors
**Solution:** Reduce context window in `launch_server.py`:
```python
ctx = 2048  # Instead of 4096
```

### Issue: Model outputs random code instead of medical text
**Cause:** Chat template mismatch (already fixed in v2)  
**Verify:** Ensure `launch_server.py` does NOT include `--chat-template gemma`

### Issue: Server slow on first request
**Normal:** First request warms up the model (~10-15s). Subsequent requests are fast.

---

## üìä Performance Benchmarks

| Hardware | First Token | Tokens/sec | Context | Model |
|----------|-------------|------------|---------|-------|
| Apple M1 8GB | ~4s | 16 tok/s | 4k | Q4_K_M |
| Apple M2 16GB | ~3s | 22 tok/s | 8k | Q4_K_M |
| NVIDIA RTX 4060 | ~2s | 35 tok/s | 8k | Q4_K_M |

---

## üìö Additional Documentation

- **Quantization Process:** See [llama.cpp/examples/quantize/](llama.cpp/examples/quantize/)
- **I-Matrix Guide:** [llama.cpp docs on importance matrix](https://github.com/ggml-org/llama.cpp/discussions/5006)
- **Model Card:** [google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)
- **Calibration Dataset:** [scripts/generate_medical_calibration_v2.py](scripts/generate_medical_calibration_v2.py)

---

## ü§ù Team Integration

### For Backend (Saad - LangGraph Agent)

Use the JSON mode for structured data:
```python
agent = MedGemmaAgent()
symptoms_data = agent.generate_strict_json(
    "Extract symptoms, vitals, and demographics from: [clinical note]"
)
# Returns: {"symptoms": [...], "vitals": {...}, "age": 45, ...}
```

### For Frontend (Othman - UI)

Use the chat mode with strong system prompts:
```python
agent = MedGemmaAgent()
response = agent.generate_clinical_text(
    user_input,
    system_prompt="You are an empathetic medical assistant. Be concise and clear.",
    max_tokens=300,
    temperature=0.3
)
```

### For Report Generation

```python
report = agent.generate_clinical_text(
    f"Generate a clinical summary for: {patient_data}",
    system_prompt="You are a medical report writer. Use formal medical language.",
    max_tokens=800,
    temperature=0.1
)
```

---

## üìù License

- **MedGemma Model:** [Gemma Terms of Use](https://ai.google.dev/gemma/terms)
- **llama.cpp:** MIT License
- **Project Code:** MIT

---

## ‚ú® Acknowledgments

- Google DeepMind for MedGemma-1.5-4B
- ggml-org for llama.cpp
- Medical datasets: ChatDoctor, MedAlpaca, MedQA-USMLE

---

## üìß Support

For issues or questions:
1. Check [Troubleshooting](#-troubleshooting) section
2. Review test results in `test_results/`
3. Contact infrastructure lead: Hamza

---

**Status:** ‚úÖ Production-Ready (as of Feb 15, 2026)
